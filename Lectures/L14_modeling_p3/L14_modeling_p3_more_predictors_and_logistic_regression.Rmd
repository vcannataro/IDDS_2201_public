---
title: "Modeling, part 3, multiple predictors again, logistic regression"
subtitle: "<br><br> IDDS 2201: Data Analytics, Module 14"
author: "Vincent L. Cannataro, Ph.D."
output:
  xaringan::moon_reader:
    css: ["../xaringan-themer.css", "../slides.css"]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
---


```{r child = "../setup.Rmd"}
```



```{r packages, echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(plotly)
library(widgetframe)
library(ggridges)
library(patchwork)
set.seed(1234)
options(dplyr.print_min = 10, dplyr.print_max = 6)
```

class: middle

# Two numerical predictors

---

## The data

```{r load-pp, message=FALSE}
pp <- read_csv(
  "data/paris-paintings.csv",
  na = c("n/a", "", "NA")
) %>%
  mutate(log_price = log(price))
```

---

## Multiple predictors

- Response variable: `log_price` 
- Explanatory variables: Width and height

```{r model-price-width-height}
pp_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Width_in + Height_in, data = pp)
tidy(pp_fit)
```

---

##  Linear model with multiple predictors

```{r model-price-width-height-tidy, echo=FALSE}
tidy(pp_fit)
```

<br>

$$\widehat{log\_price} = 4.77 + 0.0269 \times width - 0.0133 \times height$$

---

## Visualizing models with multiple predictors

.panelset[
.panel[.panel-name[Plot]
.pull-left-wide[
```{r plotly-plot, echo = FALSE, fig.align="center", warning=FALSE}
p <- plot_ly(pp,
  x = ~Width_in, y = ~Height_in, z = ~log_price,
  marker = list(size = 3, color = "lightgray", alpha = 0.5, 
                line = list(color = "gray", width = 2))) %>%
  add_markers() %>%
  plotly::layout(scene = list(
    xaxis = list(title = "Width (in)"),
    yaxis = list(title = "Height (in)"),
    zaxis = list(title = "log_price")
  )) %>%
  config(displayModeBar = FALSE)
frameWidget(p)
```
]
]
.panel[.panel-name[Code]
```{r plotly-code, eval=FALSE}
p <- plot_ly(pp,
  x = ~Width_in, y = ~Height_in, z = ~log_price,
  marker = list(size = 3, color = "lightgray", alpha = 0.5, 
                line = list(color = "gray", width = 2))) %>%
  add_markers() %>%
  plotly::layout(scene = list(
    xaxis = list(title = "Width (in)"),
    yaxis = list(title = "Height (in)"),
    zaxis = list(title = "log_price")
  )) %>%
  config(displayModeBar = FALSE)
frameWidget(p)
```
]
]

---

class: middle

# Numerical and categorical predictors

---

## Price, surface area, and living artist

- Explore the relationship between price of paintings and surface area, conditioned 
on whether or not the artist is still living
- First visualize and explore, then model
- But first, prep the data

.midi[
```{r}
pp <- pp %>%
  mutate(artistliving = if_else(artistliving == 0, "Deceased", "Living"))

pp %>%
  count(artistliving)
```
]

---

## Typical surface area

.panelset[
.panel[.panel-name[Plot]
.pull-left-narrow[
Typical surface area appears to be less than 1000 square inches (~ 80cm x 80cm). There are very few paintings that have surface area above 5000 square inches.
]
.pull-right-wide[
```{r ref.label = "viz-surf-artistliving", echo = FALSE, warning = FALSE, out.width="90%"}
```
]
]
.panel[.panel-name[Code]
```{r viz-surf-artistliving, fig.show = "hide"}
ggplot(data = pp, aes(x = Surface, fill = artistliving)) +
  geom_histogram(binwidth = 500) + 
  facet_grid(artistliving ~ .) +
  scale_fill_manual(values = c("#E48957", "#071381")) +
  guides(fill = FALSE) +
  labs(x = "Surface area", y = NULL) +
  geom_vline(xintercept = 1000) +
  geom_vline(xintercept = 5000, linetype = "dashed", color = "gray") +
  theme_bw()
```
]
]

---

## Narrowing the scope

.panelset[
.panel[.panel-name[Plot]
For simplicity let's focus on the paintings with `Surface < 5000`:

```{r ref.label = "viz-surf-lt-5000-artistliving", echo = FALSE, warning = FALSE, out.width = "55%"}
```
]
.panel[.panel-name[Code]
```{r viz-surf-lt-5000-artistliving, fig.show = "hide"}
pp_Surf_lt_5000 <- pp %>%
  filter(Surface < 5000)

ggplot(data = pp_Surf_lt_5000, 
       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +
  geom_point(alpha = 0.5) +
  labs(color = "Artist", shape = "Artist") +
  scale_color_manual(values = c("#E48957", "#071381")) + 
  theme_bw()
```
]
]

---

## Facet to get a better look

.panelset[
.panel[.panel-name[Plot]
```{r ref.label = "viz-surf-lt-5000-artistliving-facet", echo = FALSE, warning = FALSE, out.width = "80%", fig.asp = 0.5}
```
]
.panel[.panel-name[Code]
```{r viz-surf-lt-5000-artistliving-facet, fig.show = "hide"}
ggplot(data = pp_Surf_lt_5000, 
       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~artistliving) +
  scale_color_manual(values = c("#E48957", "#071381")) +
  labs(color = "Artist", shape = "Artist") + 
  theme_bw()
```
]
]

---

## Two ways to model

- **Main effects:** Assuming relationship between surface and logged price 
**does not vary** by whether or not the artist is living.
- **Interaction effects:** Assuming relationship between surface and logged 
price **varies** by whether or not the artist is living.

---

## Interacting explanatory variables

- Including an interaction effect in the model allows for different slopes, i.e. 
nonparallel lines.
- This implies that the regression coefficient for an explanatory variable would 
change as another explanatory variable changes.
- This can be accomplished by adding an interaction variable: the product of two 
explanatory variables.

---

## Two ways to model

.pull-left-narrow[
- **Main effects:** Assuming relationship between surface and logged price **does not vary** by whether or not the artist is living
- **Interaction effects:** Assuming relationship between surface and logged price **varies** by whether or not the artist is living
]
.pull-right-wide[
```{r pp-main-int-viz, echo=FALSE, out.width="70%", fig.asp = 0.9}
pp_main_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)
pp_main_fit_aug <- augment(pp_main_fit$fit)

pp_int_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)
pp_int_fit_aug <- augment(pp_int_fit$fit)

p_main <- ggplot(
  pp_main_fit_aug,
  aes(y = log_price, x = Surface, color = artistliving)
) +
  geom_point(aes(shape = artistliving), alpha = 0.5) +
  scale_x_continuous(breaks = c(0, 2500, 5000)) +
  scale_color_manual(values = c("#E48957", "#071381")) +
  geom_line(aes(y = .fitted), size = 1.5) +
  labs(y = "log_price", title = "Main effects", color = "Artist", shape = "Artist") + 
  theme_bw()

p_int <- ggplot(
  pp_int_fit_aug,
  aes(y = log_price, x = Surface, color = artistliving)
) +
  geom_point(aes(shape = artistliving), alpha = 0.5) +
  scale_x_continuous(breaks = c(0, 2500, 5000)) +
  scale_color_manual(values = c("#E48957", "#071381")) +
  geom_line(aes(y = .fitted), size = 1.5) +
  labs(y = "log_price", title = "Interaction effects", color = "Artist", shape = "Artist") + 
  theme_bw()

p_main /
  p_int  + 
  plot_layout(guides = "collect") & theme(legend.position = "bottom")
```
]

---

## Fit model with main effects

- Response variable: `log_price`
- Explanatory variables: `Surface` area and `artistliving`

.midi[
```{r model-main-effects}
pp_main_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)
tidy(pp_main_fit)
```
]

--

$$\widehat{log\_price} = 4.88 + 0.000265 \times surface + 0.137 \times artistliving$$

---

## Solving the model

- Non-living artist: Plug in 0 for `artistliving`

$\widehat{log\_price} = 4.88 + 0.000265 \times surface + 0.137 \times 0$  
$= 4.88 + 0.000265 \times surface$

--
- Living artist: Plug in 1 for `artistliving`

$\widehat{log\_price} = 4.88 + 0.000265 \times surface + 0.137 \times 1$   
$= 5.017 + 0.000265 \times surface$

---

## Visualizing main effects

.pull-left-narrow[
- **Same slope:** Rate of change in price as the surface area increases does 
not vary between paintings by living and non-living artists.
- **Different intercept:** Paintings by living artists are consistently more 
expensive than paintings by non-living artists.
]
.pull-right-wide[
```{r out.width="100%", echo = FALSE}
p_main
```
]

---

## Interpreting main effects

.midi[
```{r exp-coefs}
tidy(pp_main_fit) %>% 
  mutate(exp_estimate = exp(estimate)) %>%
  select(term, estimate, exp_estimate)
```
]

- All else held constant, for each additional square inch in painting's surface area, the price of the painting is predicted, on average, to be higher by a factor of 1.
- All else held constant, paintings by a living artist are predicted, on average, to be higher by a factor of 1.15 compared to paintings by an artist who is no longer alive.
- Paintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be 132 livres.

---

## Main vs. interaction effects

- The way we specified our main effects model only lets `artistliving` affect the intercept.
- Model implicitly assumes that paintings with living and deceased artists have the *same slope* and only allows for *different intercepts*.  

.question[
What seems more appropriate in this case?
+ Same slope and same intercept for both colors
+ Same slope and different intercept for both colors
+ Different slope and different intercept for both colors
]

---

## Interaction: `Surface * artistliving`

```{r out.width="60%", echo = FALSE}
p_int
```

---

## Fit model with interaction effects

- Response variable: log_price
- Explanatory variables: `Surface` area, `artistliving`, and their interaction

.midi[
```{r model-interaction-effects}
pp_int_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)
tidy(pp_int_fit)
```
]

---

## Linear model with interaction effects

.midi[
```{r model-interaction-effects-tidy, echo=FALSE}
tidy(pp_int_fit)
```
]

$$\widehat{log\_price} = 4.91 + 0.00021 \times surface - 0.126 \times artistliving$$
$$+ ~ 0.00048 \times surface * artistliving$$

---

## Interpretation of interaction effects

- Rate of change in price as the surface area of the painting increases does 
vary between paintings by living and non-living artists (different slopes), 
- Some paintings by living artists are more expensive than paintings by
non-living artists, and some are not (different intercept).

.small[
.pull-left[
- Non-living artist: 
$\widehat{log\_price} = 4.91 + 0.00021 \times surface$
$- 0.126 \times 0 + 0.00048 \times surface \times 0$
$= 4.91 + 0.00021 \times surface$
- Living artist: 
$\widehat{log\_price} = 4.91 + 0.00021 \times surface$
$- 0.126 \times 1 + 0.00048 \times surface \times 1$
$= 4.91 + 0.00021 \times surface$
$- 0.126 + 0.00048 \times surface$
$= 4.784 + 0.00069 \times surface$
]
.pull-right[
```{r viz-interaction-effects2, out.width="80%", echo = FALSE}
p_int
```
]
]

---

## Comparing models

It appears that adding the interaction actually increased adjusted $R^2$, so we 
should indeed use the model with the interactions.

```{r}
glance(pp_main_fit)$adj.r.squared
glance(pp_int_fit)$adj.r.squared
```

---

## Third order interactions

- Can you? Yes
- Should you? Probably not if you want to interpret these interactions in context
of the data.

---

class: middle

# Logistic regressions: predicting categorical data

---

## Spam filters

.pull-left-narrow[
- Data from 3921 emails and 21 variables on them
- Outcome: whether the email is spam or not
- Predictors: number of characters, whether the email had "Re:" in the subject, time at which email was sent, number of times the word "inherit" shows up in the email, etc.
]
.pull-right-wide[
.small[
```{r}
library(openintro)
glimpse(email)
```
]
]

---

.question[
Would you expect longer or shorter emails to be spam?
]

--

.pull-left[
```{r echo=FALSE, out.width="100%"}
email %>%
  ggplot(aes(x = num_char, y = spam, fill = spam, color = spam)) +
  geom_density_ridges2(alpha = 0.5) +
  labs(
    x = "Number of characters (in thousands)", 
    y = "Spam",
    title = "Spam vs. number of characters"
    ) +
  guides(color = FALSE, fill = FALSE) +
  scale_fill_manual(values = c("#E48957", "#CA235F")) +
  scale_color_manual(values = c("#DEB4A0", "#CA235F"))
```
]
.pull-right[
```{r echo=FALSE}
email %>% 
  group_by(spam) %>% 
  summarise(mean_num_char = mean(num_char))
```
]

---

.question[
Would you expect emails that have subjects starting with "Re:", "RE:", "re:", or "rE:" to be spam or not?
]

--

```{r echo=FALSE}
email %>%
  ggplot(aes(x = re_subj, fill = spam)) +
  geom_bar(position = "fill") +
  labs(
    x = 'Whether “re:”, "RE:", etc. was in the email subject.', 
    fill = "Spam", 
    y = NULL,
    title = 'Spam vs. "re:" in subject'
    ) +
  scale_fill_manual(values = c("#DEB4A0", "#CA235F"))
```

---

## Modeling spam

- Both number of characters and whether the message has "re:" in the subject might be related to whether the email is spam. How do we come up with a model that will let us explore this relationship?

--
- For simplicity, we'll focus on the number of characters (`num_char`) as predictor, but the model we describe can be expanded to take multiple predictors as well.

---

## Modeling spam

This isn't something we can reasonably fit a linear model to -- we need something different!

```{r echo=FALSE, out.width="70%"}
means <- email %>%
  group_by(spam) %>%
  summarise(mean_num_char = mean(num_char)) %>%
  mutate(group = 1)

ggplot(email, aes(x = num_char, y = spam, color = spam)) +
  geom_jitter(alpha = 0.2) +
  geom_line(data = means, aes(x = mean_num_char, y = spam, group = group), color = "black", size = 1.5) +
  labs(x = "Number of characters (in thousands)", y = "Spam") +
  scale_color_manual(values = c("#DEB4A0", "#CA235F")) +
  guides(color = FALSE)
```

---

## Framing the problem

- We can treat each outcome (spam and not) as successes and failures arising from separate Bernoulli trials
  - Bernoulli trial: a random experiment with exactly two possible outcomes, "success" and "failure", in which the probability of success is the same every time the experiment is conducted

--
- Each Bernoulli trial can have a separate probability of success

$$ y_i ∼ Bern(p) $$

--
- We can then use the predictor variables to model that probability of success, $p_i$

--
- We can't just use a linear model for $p_i$ (since $p_i$ must be between 0 and 1) but we can transform the linear model to have the appropriate range

---

## Generalized linear models

- This is a very general way of addressing many problems in regression and the resulting models are called **generalized linear models (GLMs)**

--
- Logistic regression is just one example

---

## Three characteristics of GLMs

All GLMs have the following three characteristics:

1. A probability distribution describing a generative model for the outcome variable

--
2. A linear model:
$$\eta = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k$$

--
3. A link function that relates the linear model to the parameter of the outcome distribution
  
---

class: middle

# Logistic regression

---

## Logistic regression

- Logistic regression is a GLM used to model a binary categorical outcome using numerical and categorical predictors

--
- To finish specifying the Logistic model we just need to define a reasonable link function that connects $\eta_i$ to $p_i$: logit function

--
- **Logit function:** For $0\le p \le 1$

$$logit(p) = \log\left(\frac{p}{1-p}\right)$$



---

## Logit function, visualized

```{r echo=FALSE}
d <- tibble(p = seq(0.001, 0.999, length.out = 1000)) %>%
  mutate(logit_p = log(p/(1-p)))

ggplot(d, aes(x = p, y = logit_p)) + 
  geom_line() + 
  xlim(0,1) + 
  ylab("logit(p)") +
  labs(title = "logit(p) vs. p")
```

---

## Properties of the logit

- The logit function takes a value between 0 and 1 and maps it to a value between $-\infty$ and $\infty$

--
- Inverse logit (logistic) function:
$$g^{-1}(x) = \frac{\exp(x)}{1+\exp(x)} = \frac{1}{1+\exp(-x)}$$

--
- The inverse logit function takes a value between $-\infty$ and $\infty$ and maps it to a value between 0 and 1

--
- This formulation is also useful for interpreting the model, since the logit can be interpreted as the log odds of a success -- more on this later

---

## The logistic regression model

- Based on the three GLM criteria we have
  - $y_i \sim \text{Bern}(p_i)$
  - $\eta_i = \beta_0+\beta_1 x_{1,i} + \cdots + \beta_n x_{n,i}$
  - $\text{logit}(p_i) = \eta_i$

--
- From which we get

$$p_i = \frac{\exp(\beta_0+\beta_1 x_{1,i} + \cdots + \beta_k x_{k,i})}{1+\exp(\beta_0+\beta_1 x_{1,i} + \cdots + \beta_k x_{k,i})}$$
---

## Modeling spam

In R we fit a GLM in the same way as a linear model except we

- specify the model with `logistic_reg()`
- use `"glm"` instead of `"lm"` as the engine 
- define `family = "binomial"` for the link function to be used in the model

--

```{r}
spam_fit <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(spam ~ num_char, data = email, family = "binomial")

tidy(spam_fit)
```

---

## Spam model

```{r}
tidy(spam_fit)
```

--

Model:
$$\log\left(\frac{p}{1-p}\right) = -1.80-0.0621\times \text{num_char}$$

---

## P(spam) for an email with 2000 characters 

$$\log\left(\frac{p}{1-p}\right) = -1.80-0.0621\times 2$$
--
$$\frac{p}{1-p} = \exp(-1.9242) = 0.15 \rightarrow p = 0.15 \times (1 - p)$$
--
$$p = 0.15 - 0.15p \rightarrow 1.15p = 0.15$$
--
$$p = 0.15 / 1.15 = 0.13$$

---

.question[
What is the probability that an email with 15000 characters is spam? What about an email with 40000 characters?
]

--

.pull-left[
```{r spam-predict-viz, echo=FALSE, out.width = "100%", fig.asp=0.5}
newdata <- tibble(
  num_char = c(2, 15, 40),
  color    = c("#A7D5E8", "#e9d968", "#8fada7"),
  shape    = c(22, 24, 23)
  )

y_hat <- predict(spam_fit, newdata, type = "raw")
p_hat <- exp(y_hat) / (1 + exp(y_hat))

newdata <- newdata %>%
  bind_cols(
    y_hat = y_hat,
    p_hat = p_hat
  )

spam_aug <- augment(spam_fit$fit) %>%
  mutate(prob = exp(.fitted) / (1 + exp(.fitted)))

ggplot(spam_aug, aes(x = num_char)) +
  geom_point(aes(y = as.numeric(spam)-1, color = spam), alpha = 0.3) +
  scale_color_manual(values = c("#DEB4A0", "#CA235F")) +
  scale_y_continuous(breaks = c(0, 1)) +
  guides(color = FALSE) +
  geom_line(aes(y = prob)) +
  geom_point(data = newdata, aes(x = num_char, y = p_hat), 
             fill = newdata$color, shape = newdata$shape, 
             stroke = 1, size = 6) +
  labs(
    x = "Number of characters (in thousands)",
    y = "Spam", 
    title = "Spam vs. number of characters"
  )
```
]
.pull-right[
- .light-blue[`r paste0(newdata$num_char[1], "K chars: P(spam) = ", round(newdata$p_hat[1], 2))`]
- .yellow[`r paste0(newdata$num_char[2], "K chars, P(spam) = ", round(newdata$p_hat[2], 2))`]
- .green[`r paste0(newdata$num_char[3], "K chars, P(spam) = ", round(newdata$p_hat[3], 2))`]
]

---

.question[
Would you prefer an email with 2000 characters to be labeled as spam or not? How about 40,000 characters?
]

```{r ref.label="spam-predict-viz", echo=FALSE, fig.asp=0.5}
```

---

# Logistic regression [in the wild](https://www.nature.com/articles/s41388-018-0657-6)

```{r, echo=FALSE, out.width="60%"}
knitr::include_graphics(path = "img/Cannataro_et_al_2019_APOBEC_log_reg.png")
```


---


class: middle

# Sensitivity and specificity

---

## False positive and negative

|                         | Email is spam                 | Email is not spam             |
|-------------------------|-------------------------------|-------------------------------|
| Email labeled spam     | True positive                 | False positive (Type 1 error) |
| Email labeled not spam | False negative (Type 2 error) | True negative                 |

--
- False negative rate = P(Labeled not spam | Email spam) = FN / (TP + FN) 

- False positive rate = P(Labeled spam | Email not spam) = FP / (FP + TN)

---

## Sensitivity and specificity

|                         | Email is spam                 | Email is not spam             |
|-------------------------|-------------------------------|-------------------------------|
| Email labeled spam     | True positive                 | False positive (Type 1 error) |
| Email labeled not spam | False negative (Type 2 error) | True negative                 |

--

- Sensitivity = P(Labeled spam | Email spam) = TP / (TP + FN)
  - Sensitivity = 1 − False negative rate
  
- Specificity = P(Labeled not spam | Email not spam) = TN / (FP + TN) 
  - Specificity = 1 − False positive rate

---

.question[
If you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision? 
]

---


# Attributions

Some of the material in this slide deck was inspired by and/or partially adapted from several open-source data science resources, including

- The [datasciencebox](https://github.com/rstudio-education/datascience-box) by Mine Çetinkaya-Rundel [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) 
- The [R for Data Science](https://r4ds.had.co.nz/) textbook by Hadley Wickham & Garrett Grolemund [CC BY-NC-ND 3.0 US](https://creativecommons.org/licenses/by-nc-nd/3.0/us/) 
- Artwork by @allison_horst [CC BY-SA 4.1](https://creativecommons.org/licenses/by-sa/4.0/) 
